{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 1 ： 在CTR点击率预估中，使用GBDT+LR的原理是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "一、模型结构及需要去解决的问题：  \n",
    "CTR 预估本质上是一个分类问题，分类问题可以想到的最基本的解决方法是 LR 模型。  \n",
    "但 CTR 预估模型中存在以下问题：\n",
    "1. 使用的数据集大；\n",
    "2. LR 是通常使用的模型，但 LR 虽然能够快速的学习，但不能做到精准的学习非线性特征；\n",
    "3. 人工的特征提取和构造成本太高，需要大量的经验。\n",
    "\n",
    "模型结构：  \n",
    "在以上的背景下，我们为了能够解决 LR 模型特征的选取问题，采用 GBDT 的树模型，先构造特征，然后使用 GBDT 树模型的叶子节点特征进行 LR 的分析。 stacking 思想 \n",
    "串行的结构： GBDT -->> LR\n",
    "\n",
    "二、GBDT 的原理：  \n",
    "GBDT 树模型采用 boosting 的思想。  \n",
    "GBDT 构造 k 个树，采用 CART 方法分裂，进行回归树建立，每个树学习上一颗树的残差，是一个总体上的串行结构。\n",
    "\n",
    "三、LR 模型中的特征选择：正则化项  \n",
    "由于自动提取的特征可能过多（树的叶子节点），使用正则化项惩罚过多的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 2 ：Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "Wide & Deep 模型分为两个部分，一部分为 Wide（线性部分）：强调记忆性；一部分为 Deep （非线性神经网络部分）：强调模型的泛化能力。  \n",
    "1. Wide 线性回归部分，直接使用原始的特征以及原始特征的交叉项，只要原始特征给出，就可以给它在模型中体现出来，强调记忆性，构造的特征和选择的特征均为人工提取的特征。\n",
    "2. Deep 非线性神经网络部分， 这部分使用的特征需要先进行 embedding，然后使用神经网络进行非线性的学习，其学习的是非线性部分，得到的是前面没出现过的特征，因此强调其泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 3 ：在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "Deep FM ；NFM \n",
    "1. Deep FM 使用并行的方式，相当于将Wide& Deep中的 LR 替换为了 FM。\n",
    "2. NFM 采用串行的方式，在使用 FM 继续宁二阶特征提取后，再进行神经网络的拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 4 ：GBDT和随机森林都是基于树的算法，它们有什么区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "GBDT 和 随机森林 都是集成学习的方法，但有些许的不同：    \n",
    "1. GBDT 是 Boosting 思想，采用串行的方式，将多个分类器集成在一起，每一个弱分类器是对上一个分类器的残差进行分析。  \n",
    "2. RF 随机森林是 Bagging 思想，采用并行的方式将多个弱分类器集成在一起，通过投票的方式，采用多数的结果为最后结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 5 ： item流行度在推荐系统中有怎样的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "1. 商品流行度解决冷启动问题：当没有用户太多的行为时，使用热门商品进行推荐。\n",
    "2. 改进用户之间相似度问题：流行商品不好体现用户的个性化特征，因此使用基于流行度的权重对两个用户之间的相似度进行加权。\n",
    "3. 改进商品的推荐次序：流行商品由于不好体现用户个性化特征，因此再推荐时，对商品基于流行度进行降权，较少推荐流行商品。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action 1 ： 使用Wide&Deep模型对movielens进行评分预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples, validate on 32 samples\n",
      "128/128 [==============================] - 1s 5ms/sample - loss: 14.6953 - mean_squared_error: 14.6953 - val_loss: 12.9068 - val_mean_squared_error: 12.9068\n",
      "test RMSE 3.630592238189246\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from deepctr.models import WDL, NFM\n",
    "from deepctr.feature_column import SparseFeat,get_feature_names\n",
    "\n",
    "data = pd.read_csv(\"movielens_sample.txt\")\n",
    "sparse_features = [\"movie_id\", \"user_id\", \"gender\", \"age\", \"occupation\", \"zip\"]\n",
    "target = ['rating']\n",
    "\n",
    "# 对特征标签进行编码\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature])\n",
    "    \n",
    "# 计算每个特征中的 不同特征值的个数\n",
    "fixlen_feature_columns = [SparseFeat(feature, data[feature].nunique()) for feature in sparse_features]\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "# 将数据集切分成训练集和测试集\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train_model_input = {name:train[name].values for name in feature_names}\n",
    "test_model_input = {name:test[name].values for name in feature_names}\n",
    "\n",
    "# 使用WDL进行训练\n",
    "model = WDL(linear_feature_columns, dnn_feature_columns, task='regression')\n",
    "model.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "history = model.fit(train_model_input, train[target].values, batch_size=256, epochs=1, verbose=True, validation_split=0.2, )\n",
    "# 使用WDL进行预测\n",
    "pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "# 输出RMSE或MSE\n",
    "mse = round(mean_squared_error(test[target].values, pred_ans), 4)\n",
    "rmse = mse ** 0.5\n",
    "print(\"test RMSE\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 128 samples, validate on 32 samples\n",
      "128/128 [==============================] - 1s 6ms/sample - loss: 14.6953 - mean_squared_error: 14.6953 - val_loss: 12.9123 - val_mean_squared_error: 12.9123\n",
      "test RMSE 3.631294535010896\n"
     ]
    }
   ],
   "source": [
    "# 使用NFM进行训练\n",
    "model2 = NFM(linear_feature_columns, dnn_feature_columns, task='regression')\n",
    "model2.compile(\"adam\", \"mse\", metrics=['mse'], )\n",
    "history2 = model2.fit(train_model_input, train[target].values, batch_size=256, epochs=1, verbose=True, validation_split=0.2, )\n",
    "# 使用NFM进行预测\n",
    "pred_ans2 = model2.predict(test_model_input, batch_size=256)\n",
    "# 输出RMSE或MSE\n",
    "mse2 = round(mean_squared_error(test[target].values, pred_ans2), 4)\n",
    "rmse2 = mse2 ** 0.5\n",
    "print(\"test RMSE\", rmse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本章任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "[text:'原理', text:'GBDT+LR的原理']\n",
      "[text:'原理', text:'Gradient Boosting原理']\n",
      "[text:'原理', text:'使用GBDT进行新特征构造']\n",
      "[text:'原理', text:'评价指标NE，Calibration，AUC']\n",
      "[text:'原理', text:'TPRate，FPRate计算']\n",
      "[text:'工具', text:'sklearn中的GBDT']\n",
      "[text:'原理', text:'Wide & Deep模型']\n",
      "[text:'原理', text:'Wide join Deep同时训练']\n",
      "[text:'工具', text:'DeepCTR中的WDL']\n",
      "[text:'原理', text:'FNN模型']\n",
      "[text:'原理', text:'NFM模型']\n",
      "[text:'原理', text:'FM与DNN的两种结合方式：DeepFM，NFM']\n",
      "[text:'工具', text:'DeepCTR中的NFM']\n",
      "[text:'原理', text:'基于流行度的推荐算法']\n",
      "[text:'原理', text:'影响流行度的因素']\n",
      "[text:'原理', text:'推荐系统的架构']\n",
      "[text:'Thinking1', text:'在CTR点击率预估中，使用GBDT+LR的原理是什么？'] text:'能简要说明GBDT和LR在CTR预估中的作用（10point）'\n",
      "[text:'Thinking2', text:'Wide & Deep的模型结构是怎样的，为什么能通过具备记忆和泛化能力（memorization and generalization）'] text:'1、能简要说明Wide&Deep的模型（5point）\\n2、如何具备记忆和泛化能力（5point）\\n'\n",
      "[text:'Thinking3', text:'在CTR预估中，使用FM与DNN结合的方式，有哪些结合的方式，代表模型有哪些？'] text:'1、能说出哪两种FM和DNN的组合方式（5points）\\n2、能说出FM和DNN的组合出的算法（5points）'\n",
      "[text:'Thinking4', text:'GBDT和随机森林都是基于树的算法，它们有什么区别？'] text:'能简要说明这两种基于树的算法的不同（10points）'\n",
      "[text:'Thinking5', text:'item流行度在推荐系统中有怎样的应用'] text:'1、冷启动中的使用（10points)\\n2、协同过滤中的TopN推荐（10points）\\n3、其他使用（+5points)'\n",
      "[text:'Action1', text:'使用Wide&Deep模型对movielens进行评分预测'] text:'1、完成代码（20points）\\n2、使用Wide&Deep工具进行评分预测，代码正确（20points）'\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "data = xlrd.open_workbook('L5-2自测文档.xls')\n",
    "#通过索引顺序获取\n",
    "table = data.sheet_by_index(0)\n",
    "\n",
    "\"\"\" 工作表中行/列的操作 \"\"\"\n",
    "#获取该sheet中的有效行数\n",
    "nrows = table.nrows  \n",
    "print(nrows)\n",
    "row_index, col_index = 0, 0\n",
    "# 获取某行信息\n",
    "for row_index in range(2, nrows-7):\n",
    "    print(table.row(row_index)[:2])\n",
    "for row_index in range(nrows-6, nrows):\n",
    "    print(table.row(row_index)[:2], table.row(row_index)[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
