{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 1 ：什么是反向传播中的链式法则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "最终结果的误差是由前面几层神经网络传递而来，要想求得每一层的误差，就需要使用每一层后面一层的误差乘上导数，每一层都是如此，中间不可以中断，一直到第一层神经网络，会发现第一层神经网络权重的修正值是后面几层权重转置累乘与最终误差的乘积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 2 ： 请列举几种常见的激活函数，激活函数有什么作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "ReLU；Sigmoid；tanh，激活函数的作用是增加神经网络的非线性特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 3 ：利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "1. 权重未更新 -->> 使用SGD, Adam 等优化方法对去权重进行更新。\n",
    "2. 梯度消失，权重每次更新的幅度太小 -->> 更换激活函数，使用 ReLU 避免发生梯度消失现象。\n",
    "3. 使用梯度下降时，学习率 Learning rate 太小，每次更新参数都不明显 -->> 使用Adam 自适应更新参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action 1 :使用Pytorch编写神经网络，完成boston房价预测问题\n",
    ">1）数据加载：from sklearn.datasets import load_boston  \n",
    "2）网络结构：  \n",
    "l1 = Linear(X, W1, b1)  \n",
    "s1 = Relu(l1)  \n",
    "l2 = Linear(s1, W2, b2)  \n",
    "cost = MSE(y, l2)   \n",
    "隐藏层维度为10  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、使用 Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:\n",
      " [[ 4.60356869e-01 -1.53199023e+00 -1.59860248e+00 -5.24377603e-01\n",
      "   1.56953777e-01  1.17024836e+00 -2.57409734e-01  1.04251780e+00\n",
      "  -3.57124895e-01  3.84586059e-01 -5.27868979e-01 -8.16054582e-01\n",
      "  -6.42512810e-01  3.64150817e-01 -1.17206429e+00 -2.06433198e+00]\n",
      " [-1.07408569e+00  1.12234293e+00 -8.23750680e-01  5.49637220e-01\n",
      "   1.13567292e+00  7.44743607e-01 -2.22452963e+00 -2.24000088e-01\n",
      "  -6.37504574e-01 -4.54058795e-01 -7.33122863e-01  3.41927116e-01\n",
      "   8.91042451e-01 -7.36156525e-01 -1.23111841e+00 -8.63705200e-01]\n",
      " [ 1.27801219e+00  3.84091231e-01 -8.33583288e-01  5.30926906e-01\n",
      "  -3.48377188e-01 -4.74753000e-01 -6.29172584e-01  9.14809573e-02\n",
      "  -2.58940048e-01  7.05577955e-01  3.86551121e-01 -4.73106507e-01\n",
      "  -9.40843835e-01  2.58882823e-01 -1.07338185e+00 -4.26696606e-01]\n",
      " [-2.51740430e-01  1.27160351e+00 -8.94590546e-01 -1.91210710e+00\n",
      "   1.56786245e+00  2.18709786e-01 -5.13801092e-01  5.55945420e-01\n",
      "  -1.01576823e+00  7.48400869e-01  2.15906380e+00 -8.36850662e-01\n",
      "   1.28975345e+00 -2.87141454e-01  3.48890840e-01 -2.90148698e+00]\n",
      " [-1.53514027e+00  7.57403931e-01  1.08625551e+00 -6.87602964e-02\n",
      "  -1.23836452e+00  1.84646127e+00  1.58932312e+00 -2.46147822e+00\n",
      "   9.38992417e-01  5.64254557e-01  5.64551625e-02 -1.29023494e+00\n",
      "   6.40215763e-01  7.66961825e-01 -5.48276755e-01  2.10034705e+00]\n",
      " [ 9.85601023e-01 -2.25028381e-01  1.64708964e+00 -2.26084406e-01\n",
      "   2.20682643e+00 -1.22881477e+00  7.49553599e-01 -1.19899385e+00\n",
      "   1.87702455e+00  9.47700633e-01 -8.49615192e-01  1.11955899e+00\n",
      "  -8.11197439e-01 -5.39147499e-01 -4.62491575e-02  1.97296595e-01]\n",
      " [ 4.24192293e-01  9.63966493e-01  2.80849589e+00  9.62087579e-01\n",
      "  -5.58589853e-01  4.19027485e-02 -2.48835821e-01  8.65256865e-01\n",
      "  -4.42922546e-01  1.67789136e+00  5.85737335e-01 -7.55772602e-01\n",
      "  -5.48332979e-01  1.02644763e+00  1.58814706e-01 -1.30942469e+00]\n",
      " [-1.01261396e+00 -7.82988878e-01  3.82421629e-01 -4.46722241e-01\n",
      "  -9.67839929e-01 -4.59113317e-04 -2.70262513e+00 -1.12629567e+00\n",
      "   1.27088680e+00  7.49795148e-01 -2.10528982e+00 -9.16909319e-01\n",
      "  -1.40244973e+00  1.92606000e+00  6.09022938e-01  1.22711714e+00]\n",
      " [ 2.01850643e+00  1.41720591e-01 -7.75784726e-01 -6.10973074e-01\n",
      "   2.95946907e-01 -2.35617693e+00  1.31365053e+00  5.09926253e-01\n",
      "  -1.54680731e+00  1.02257934e+00  1.28176803e-01  7.24424893e-01\n",
      "  -2.47214426e-01 -7.40553984e-01 -1.17120749e+00 -1.00808273e+00]\n",
      " [-3.91808579e-01 -2.17570591e+00  5.95294107e-01 -1.50319111e+00\n",
      "  -5.86923900e-01 -1.51340981e-01 -1.43932471e+00 -1.08889703e+00\n",
      "   8.20948649e-01 -1.99407249e+00 -1.38476100e+00 -5.85606655e-01\n",
      "   1.42505032e-01 -1.75188860e+00 -1.51540356e+00  1.74680717e+00]\n",
      " [ 6.13034387e-01  1.76130986e-01 -6.24535730e-01  1.17402830e+00\n",
      "   1.67287089e-01  1.78581153e-01  1.60712710e-01  1.53684720e+00\n",
      "  -2.41092350e+00  1.89339553e+00 -1.83006077e+00 -7.39726485e-01\n",
      "   9.26624569e-01  1.46764941e-01  1.20112073e+00  1.71277725e+00]\n",
      " [ 9.88023261e-02  5.35977562e-01  6.63079103e-01  6.96350916e-01\n",
      "  -9.34858336e-01 -2.54042587e-01 -2.99397652e-01  5.48274326e-04\n",
      "   9.74163450e-01 -1.21656523e+00 -2.88352602e-01 -5.43803506e-02\n",
      "  -1.79557633e-01 -9.74736981e-01 -8.54767811e-01  6.28412400e-01]\n",
      " [-1.40997808e+00  1.65007080e+00 -9.92443946e-01 -1.73676379e+00\n",
      "  -1.23470200e+00 -8.64825637e-01  9.18959193e-01 -1.58560694e-01\n",
      "   5.55574588e-01  7.07659755e-01 -1.38621593e+00  9.44096401e-01\n",
      "  -1.42297456e-01  6.78555449e-01 -2.35995232e-01  5.69225384e-02]]\n",
      "W2:\n",
      " [[ 2.50660241]\n",
      " [-1.23868192]\n",
      " [-0.42948488]\n",
      " [ 2.00689304]\n",
      " [ 1.13339562]\n",
      " [-2.89523803]\n",
      " [ 0.66657614]\n",
      " [ 1.56860011]\n",
      " [ 1.57926347]\n",
      " [-1.09580925]\n",
      " [ 1.65484027]\n",
      " [ 1.60694395]\n",
      " [ 1.74597388]\n",
      " [ 1.98067265]\n",
      " [-0.67392274]\n",
      " [ 1.49813903]]\n",
      "MSE = 15.957219565886861\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#加载数据\n",
    "data = load_boston()\n",
    "target = data.target\n",
    "data = data.data\n",
    "# print(data.shape)\n",
    "\n",
    "#数据标准化\n",
    "# ss = StandardScaler\n",
    "# X_2 = ss(data).copy\n",
    "X_ = data\n",
    "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
    "\n",
    "\n",
    "# 目标值维度转换\n",
    "y_ = target.reshape(target.shape[0], 1)\n",
    "# print(y_.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_, y_, test_size = 0.25, random_state = 33)\n",
    "\n",
    "#定义输入变量\n",
    "X = x_train\n",
    "y = y_train\n",
    "\n",
    "import numpy as np\n",
    "#定义维度\n",
    "n, d_in, d_hidden, d_out = x_train.shape[0], x_train.shape[1], 16, 1\n",
    "def linear(X, W, b):\n",
    "    result = X.dot(W) + b\n",
    "    return result\n",
    "\n",
    "def Relu(y):\n",
    "    return np.where(y < 0, 0, y)\n",
    "\n",
    "def loss(y, y_pre):\n",
    "    return np.mean(np.square(y - y_pre))\n",
    "    \n",
    "#构建两层神经网络 W1，b1  W2，b2  Relu \n",
    "def init_net():\n",
    "    #初始化神经网络\n",
    "    np.random.seed(33)\n",
    "    W1 = np.random.randn(d_in, d_hidden)\n",
    "    b1 = np.random.randn(d_hidden)\n",
    "    W2 = np.random.randn(d_hidden, d_out)\n",
    "    b2 = np.random.randn(d_out)\n",
    "    return W1,W2,b1,b2\n",
    "    \n",
    "def forward(X, W1, W2, b1, b2):\n",
    "    #线性层\n",
    "    y1 = linear(X, W1, b1)\n",
    "    ReLU = Relu(y1)\n",
    "    y2 = linear(ReLU, W2, b2)\n",
    "    return y1, ReLU, y2\n",
    "\n",
    "#定义神经网络，返回值为权重\n",
    "def NPnet(X, y, learning_rate = 1e-5, iter_num = 5000):\n",
    "    W1, W2, b1, b2 = init_net()\n",
    "    loss_temp = 0.0\n",
    "    for i in range(iter_num):\n",
    "        y1, ReLU, y_pre = forward(X, W1, W2, b1, b2)\n",
    "        mse_loss = loss(y, y_pre)\n",
    "        #print(mse_loss)\n",
    "        #print(loss_temp)\n",
    "        #早停法\n",
    "        if (i > 0 and (np.abs(mse_loss - loss_temp) < 1e-10)) or (i == iter_num - 1):\n",
    "            #print(mse_loss - loss_temp < 1e-10)\n",
    "            return W1, W2, b1, b2, y_pre, i\n",
    "        \n",
    "        loss_temp = mse_loss.copy()\n",
    "#         print(np.mean(y_pre - y))\n",
    "        #输出层梯度\n",
    "        grad_y_pre = 2.0 * (y_pre - y)\n",
    "        #print(grad_y_pre.shape)\n",
    "        #print(grad_y_pre)\n",
    "        \n",
    "        # W2 梯度更新\n",
    "        grad_W2 = ReLU.T.dot(grad_y_pre)\n",
    "        grad_b2 = grad_y_pre.sum(axis = 0)\n",
    "        \n",
    "        # 第二层(W2) 传递到的 Loss 值\n",
    "        loss_W2 = grad_y_pre.dot(W2.T)\n",
    "        #print(grad_Relu.shape)\n",
    "        #print(y1.shape)\n",
    "        #print(grad_W2.shape)\n",
    "        #print(W2.shape)\n",
    "\n",
    "        # ReLU 层梯度更新\n",
    "        loss_ReLU = loss_W2.copy()\n",
    "        loss_ReLU[y1 < 0] = 0\n",
    "        \n",
    "        #print(loss_ReLU)\n",
    "        #print(loss_W2)\n",
    "        \n",
    "        # W1 更新权重\n",
    "        #print(X)\n",
    "        grad_W1 = X.T.dot(loss_ReLU)\n",
    "        #print(grad_W1)\n",
    "        grad_b1 = loss_ReLU.sum(axis = 0)\n",
    "        #print(grad_b1.shape)\n",
    "        W1 -= learning_rate * grad_W1\n",
    "        \n",
    "        # 更新 W2 \n",
    "        W2 -= learning_rate * grad_W2\n",
    "        b2 -= learning_rate * grad_b2\n",
    "        #print(W2)\n",
    "        b1 -= learning_rate * grad_b1\n",
    "        #print(W1)\n",
    "        \n",
    "\n",
    "W1, W2, b1, b2, y_hat, i = NPnet(X, y)\n",
    "print(\"W1:\\n\", W1)\n",
    "print(\"W2:\\n\", W2)\n",
    "# print(y)\n",
    "# print(y_pre.reshape(1,y_pre.shape[0]))\n",
    "# print(grad_y_pre)\n",
    "# print(i)\n",
    "# print(loss(y, y_hat))\n",
    "\n",
    "y1, ReLU, y_pre = forward(x_test, W1, W2, b1, b2)\n",
    "# print(y_pre, y_pre.shape)\n",
    "print(\"MSE = {}\".format(loss(y_test, y_pre)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用Numpy 构造的神经网络学习率太大时容易溢出，且梯度下降法不如 Adam 的优化方式，不容易调试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、使用 pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-d35c46f8d1a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpytorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pytorch'"
     ]
    }
   ],
   "source": [
    "import pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Action 2: 对移动推荐系统进行可视化数据探索\n",
    ">数据集https://tianchi.aliyun.com/competition/entrance/231522/information\\ntianchi_fresh_comp_train_item.csv\\ntianchi_fresh_comp_train_user.csv  \n",
    "比如时间规律统计，4种行为类别的对比…  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本章任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "[text:'原理', text:'神经网络结构']\n",
      "[text:'原理', text:'激活函数']\n",
      "[text:'原理', text:'损失函数']\n",
      "[text:'原理', text:'反向传播']\n",
      "[text:'原理', text:'梯度下降']\n",
      "[text:'原理', text:'优化方法（SGD、Adam）']\n",
      "[text:'工具', text:'使用numpy搭建神经网络']\n",
      "[text:'工具', text:'使用pytorch搭建神经网络']\n",
      "[text:'原理', text:'Project：移动推荐系统']\n",
      "[text:'原理', text:'探索性数据分析']\n",
      "[text:'原理', text:'灵活使用dataframe']\n",
      "[text:'原理', text:'时间函数使用']\n",
      "[text:'工具', text:'分块读取海量数据']\n",
      "[text:'工具', text:'特征工程']\n",
      "[text:'Thinking1', text:'什么是反向传播中的链式法则'] text:'简要说明反向传播中的链式法则（10points）'\n",
      "[text:'Thinking1', text:'请列举几种常见的激活函数，激活函数有什么作用'] text:'简要说明常用的激活函数及作用（10points）'\n",
      "[text:'Thinking2', text:'利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？'] text:'能简要说明loss不变的解决方案（10points）'\n",
      "[text:'Action1', text:'使用Pytorch编写神经网络，完成boston房价预测问题\\n1）数据加载：from sklearn.datasets import load_boston\\n2）网络结构：\\nl1 = Linear(X, W1, b1)\\ns1 = Relu(l1)\\nl2 = Linear(s1, W2, b2)\\ncost = MSE(y, l2)\\n隐藏层维度为10\\n'] text:'1、完成代码（20points）\\n2、结果正确（10points）\\n'\n",
      "[text:'Action2', text:'对移动推荐系统进行可视化数据探索\\n数据集https://tianchi.aliyun.com/competition/entrance/231522/information\\ntianchi_fresh_comp_train_item.csv\\ntianchi_fresh_comp_train_user.csv\\n比如时间规律统计，4种行为类别的对比…'] text:'1、完成代码（20points）\\n2、结果正确（20points）\\n'\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "data = xlrd.open_workbook('L7自测文档.xls')\n",
    "#通过索引顺序获取\n",
    "table = data.sheet_by_index(0)\n",
    "\n",
    "\"\"\" 工作表中行/列的操作 \"\"\"\n",
    "#获取该sheet中的有效行数\n",
    "nrows = table.nrows  \n",
    "print(nrows)\n",
    "row_index, col_index = 0, 0\n",
    "# 获取某行信息\n",
    "for row_index in range(2, nrows-6):\n",
    "    print(table.row(row_index)[:2])\n",
    "for row_index in range(nrows-5, nrows):\n",
    "    print(table.row(row_index)[:2], table.row(row_index)[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
