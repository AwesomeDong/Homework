{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 1 ：什么是近似最近邻查找，常用的方法有哪些"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "一、近邻算法有两种：KNN（K - Nearest Neighbor） 与 ANN（approximate - Nearest Neighbor），相比于 KNN 精确但复杂度高的查找方式（计算所有距离取前 k 个），ANN 使用近似的距离查找邻居，牺牲一定的精度但时间复杂度更低。ANN 即为近似最近邻查找。\n",
    "\n",
    "二、近似最近邻查找常用的方法有：LSH（局部敏感 Hash）、LSHForest、LSHensembel、SimHash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 2 ：为什么两个集合的minhash值相同的概率等于这两个集合的Jaccard相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "一、设两列中元素有以下几种关系：\n",
    ">1. a : (1, 1)\n",
    "2. b : (1, 0)\n",
    "3. c : (0, 0)  \n",
    "\n",
    "二、Jaccard 相似度：交比并\n",
    "-->> $ \\frac{a}{a+b} $\n",
    "\n",
    "三、MinHash： 每个个体第一次出现（1）这个情况的行号  \n",
    "\n",
    "四、重复采样 m 次后（变换 m 次后），两列 MinHash 相等的概率：   \n",
    ">1. 行特征值的多次重复随机（变换行号，每一列出现在第一列的概率相同，充分考虑每一个特征在第一列的情况，如果 m 足够大，则可以遍历所有 a，b 的可能性），  \n",
    "2. 两列相等的概率，就等于第一列相等（1，1）的概率，因为MinHash 不考虑（0，0）情况，第一列相等的概率为 (1,1) / [(1,1) + (1,0) + (0,1)]：$ \\frac{a}{a+b} $  \n",
    "\n",
    "这样，就通过概率值将 Jaccard 相似度转化为了近似的概率问题（m 次重复实验，两列 MinHash 相等的概率）并且为了满足降维的需求，这里的 m 一定小于原特征数量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 3 ：SimHash在计算文档相似度的作用是怎样的？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "一、使用的相似度计算规则：汉明距离 -->> 两个二进制串中不同位置的数量（汉明距离越小，相似度越高）  \n",
    "\n",
    "二、SimHash 的编码规则：\n",
    ">1. 首先对文档进行分词，得到文档特征（N- Gram 等）；\n",
    "2. 初始化 SimHash 编码位数为 0 ；\n",
    "3. 对每篇文档的特征计算其重要程度（即为权重 w）；\n",
    "4. 对每篇文档的特征使用 hash 函数编码成二进制串（SimHash相同位数）；\n",
    "5. 对每篇文档特征的二进制串与权重相乘，规则为 0 代表 -1 、1 代表 1 ，得到每个特征的编码；\n",
    "6. 对每篇文档特征的编码对应位置求和，并将负值转化为 0 ，正值转化为 1 ，即为每篇文档最后的 SimHash 编码；\n",
    "\n",
    "三、对于大数据集，需要进行降维：\n",
    ">1. 抽屉原理：一般我们认为汉明距离在 3 以内即为相似的，因此将编码分为 4 类，则相似的文档至少有一类是相似的；\n",
    "2. 对四类中的文档进行精确的匹配（相等概率为$1/2^{16}$），得到相应的候选集：10亿数据集大约会得到 100 万（$2^{34-16}$）的候选集\n",
    "\n",
    "四、使用汉明距离对候选集中的每篇文档的 SimHash 二进制编码进行相似度计算。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 4 ：为什么YouTube采用期望观看时间作为评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "取决于商业目标：  \n",
    "因为 YouTube 是一个视频网站，用户喜欢一个视频大概率会观看时间更长，所以不能仅仅使用 CTR 来进行预测，需要进行 观看时间 作为评估指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action 1 ：使用MinHashLSHForest对微博新闻句子进行检索 weibo.txt\n",
    ">针对某句话进行Query，查找Top-3相似的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查找句：(国足输给叙利亚后，里皮坐不住了，直接辞职了) 的相似句\n",
      "[2, 42, 46]\n",
      "相似句2：国足 输给 叙利亚 之后 里皮 辞职  \n",
      " 与原句相似度：1.0\n",
      "相似句42：国足 输给 叙利亚 里皮 坐不住 直接 辞职  \n",
      " 与原句相似度：1.0\n",
      "相似句46：国足 昨晚 输给 叙利亚 赛后 主帅 里皮 宣布 辞职  \n",
      " 与原句相似度：1.0\n"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHash, MinHashLSH, MinHashLSHForest\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import re\n",
    "\n",
    "#文件读取\n",
    "f = open('./weibos.txt', 'r', encoding='UTF-8')\n",
    "text = f.read()\n",
    "# 以句号，叹号，问号作为分隔，去掉\\n换行符号\n",
    "weibos = re.split('[。！？]', text.replace('\\n', '。'))\n",
    "#print(weibos)\n",
    "\n",
    "# 最后一行如果为空，则删除\n",
    "weibos = [weibo for weibo in weibos if weibo != \"\"]\n",
    "\n",
    "with open('chinese_stopwords.txt', 'r', encoding = 'utf-8') as file:\n",
    "    #去掉换行符\n",
    "    stop_words = [i[:-1]for i in file.readlines()]\n",
    "# print(weibos)\n",
    "\n",
    "#分词\n",
    "def get_corpus(words):\n",
    "    corpus = str()\n",
    "    for word in list(words):\n",
    "        if word.word not in stop_words:\n",
    "            corpus += str(word.word + ' ')\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def get_all_corpus(weibos):\n",
    "    all_corpus = []\n",
    "    for weibo in weibos:\n",
    "        words = pseg.cut(weibo)\n",
    "        corpus = get_corpus(words)\n",
    "        all_corpus.append(corpus)\n",
    "    all_corpus = [corpus for corpus in all_corpus if corpus != '']\n",
    "    return all_corpus\n",
    "\n",
    "\n",
    "#创建MinHash\n",
    "def get_mh(corpus):\n",
    "    mh = MinHash()\n",
    "    for word in corpus:\n",
    "        mh.update(word.encode('utf-8'))\n",
    "        return mh\n",
    "\n",
    "def get_forest(all_corpus):\n",
    "    forest = MinHashLSHForest()\n",
    "    minhash_list = []\n",
    "    i = 0\n",
    "    for corpus in all_corpus:\n",
    "        mh = get_mh(corpus)\n",
    "        minhash_list.append(mh)\n",
    "        forest.add(i, mh)\n",
    "        i += 1\n",
    "    return forest, minhash_list\n",
    "\n",
    "jieba.add_word(\"里皮\")\n",
    "jieba.add_word(\"国足\")\n",
    "all_corpus = get_all_corpus(weibos)\n",
    "# print(all_corpus)\n",
    "forest, minhash_list = get_forest(all_corpus)\n",
    "# print(len(minhash_list), len(all_corpus))\n",
    "\n",
    "\n",
    "#建立索引\n",
    "forest.index()\n",
    "\n",
    "query = \"国足输给叙利亚后，里皮坐不住了，直接辞职了\"\n",
    "print(\"查找句：({}) 的相似句\".format(query))\n",
    "query_corpus = get_corpus(pseg.cut(query))\n",
    "# print(query_corpus)\n",
    "query_mh = get_mh(query_corpus)\n",
    "\n",
    "results = forest.query(query_mh, 3)\n",
    "print(results)\n",
    "for result in results:\n",
    "    print('相似句{}：{} \\n 与原句相似度：{}'.format(result, all_corpus[result], query_mh.jaccard(minhash_list[result])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('chinese_stopwords.txt', 'r', encoding = 'utf-8') as file:\n",
    "#     stopwords = [i[:-1]for i in file.readlines()]#去掉换行符  \\n\n",
    "# print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本章任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "[text:'原理', text:'什么是近似最近邻查找，常用的方法有哪些']\n",
      "[text:'原理', text:'什么是Hash函数']\n",
      "[text:'原理', text:'什么是k-shingle']\n",
      "[text:'原理', text:'最小哈希值']\n",
      "[text:'原理', text:'MinHash执行']\n",
      "[text:'原理', text:'MinHashLSH']\n",
      "[text:'原理', text:'MinHash与Jaccard相似度']\n",
      "[text:'工具', text:'DataSketch中的MinHash, MinHashLSH, MinHashLSHForest, MinHashLSHEnsemble']\n",
      "[text:'工具', text:'如何使用MinHashLSHForest对新闻句子Top-K相似内容进行查找']\n",
      "[text:'原理', text:'汉明（Hamming）距离']\n",
      "[text:'原理', text:'SimHash算法流程']\n",
      "[text:'原理', text:'如何通过SimHash算法得到每篇文档指纹']\n",
      "[text:'工具', text:'如何通过SimHash工具计算两篇文档的Hamming距离']\n",
      "[text:'原理', text:'YouTube召回阶段DNN（输入、隐藏层、输出）']\n",
      "[text:'原理', text:'YouTube排序阶段DNN（输入、隐藏层、输出）']\n",
      "[text:'原理', text:'正负样本和上下文选择']\n",
      "[text:'原理', text:'负采样 Negative Sampling']\n",
      "[text:'原理', text:'为什么YouTube采用期望观看时间作为评估指标']\n",
      "[text:'原理', text:'为什么YouTube在排序阶段没有采用经典的LR（逻辑回归）当作输出层，而是采用了Weighted Logistic Regression？']\n",
      "[text:'Thinking1', text:'什么是近似最近邻查找，常用的方法有哪些'] text:'能简要说明近似最近邻查找（5point）\\n常用的方法（5point）'\n",
      "[text:'Thinking2', text:'为什么两个集合的minhash值相同的概率等于这两个集合的Jaccard相似度'] text:'能简要说明MinHash值相同的概率与Jaccard相似度相等的证明（10point）\\n\\n'\n",
      "[text:'Thinking3', text:'SimHash在计算文档相似度的作用是怎样的？'] text:'1、文档SimHash的计算过程（5points）\\n2、如何通过文档的SimHash计算文档之间的相似度（5points）'\n",
      "[text:'Thinking4', text:'为什么YouTube采用期望观看时间作为评估指标'] text:'能简要说明原因（10points）'\n",
      "[text:'Action1', text:'使用MinHashLSHForest对微博新闻句子进行检索 weibo.txt\\n针对某句话进行Query，查找Top-3相似的句子'] text:'1、完成代码（30points）\\n2、使用MinHashLSHForest工具对MinHash进行index，并完成Query Top-K，代码正确（30points）'\n"
     ]
    }
   ],
   "source": [
    "import xlrd\n",
    "data = xlrd.open_workbook('L6-2自测文档.xls')\n",
    "#通过索引顺序获取\n",
    "table = data.sheet_by_index(0)\n",
    "\n",
    "\"\"\" 工作表中行/列的操作 \"\"\"\n",
    "#获取该sheet中的有效行数\n",
    "nrows = table.nrows  \n",
    "print(nrows)\n",
    "row_index, col_index = 0, 0\n",
    "# 获取某行信息\n",
    "for row_index in range(2, nrows-6):\n",
    "    print(table.row(row_index)[:2])\n",
    "for row_index in range(nrows-5, nrows):\n",
    "    print(table.row(row_index)[:2], table.row(row_index)[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
