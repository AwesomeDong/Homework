{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinkeing 1 :关联规则中的支持度、置信度和提升度代表的什么，如何计算?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：   \n",
    "1. 支持度：每个商品（商品组合）在总体购物小票中的出现概率：  $ Support_i = \\frac{包含 商品 （i） 的小票个数}{总体小票个数} $   \n",
    "\n",
    "2. 置信度：当某一商品（商品组合） j 购买时，另一个其他商品（商品组合） i 会购买的概率： $ Confidence(i|j) = \\frac{商品（i，j）同时出现的小票数量}{商品（j）出现的小票数量} $  \n",
    "\n",
    "3. 支持度：当某一组合 （i | j） 组合售卖时，j 的出现对 i 商品售卖的提升程度：$ Lift(i|j) = \\frac{Confidence(i|j)}{Support(i)}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 2 : 关联规则与协同过滤的区别?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "1. 关联规则是静态的，其基于所有的整体数据，没有用到具体用户的数据，超市只有一个超市，没有冷启动问题。  \n",
    "\n",
    "2. 协同过滤是动态的过程，基于用户（UserCF）之间，商品（ItemCF）之间的相似度，为用户提供更好的个性化商场，强调“千人千面”，每个人都有不同的商场，在数据量不够大时，存在冷启动问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 3 : 为什么我们需要多种推荐算法?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：  \n",
    "1. 首先对于推荐系统的 ‘EE’ 问题，需要我们使用多种推荐算法：Exploit可以充分挖掘已有信息，但会造成信息茧房问题；Explore 可以猜测用户的其他新兴趣，但准确率较低，猜测时必然会存在误差。为了满足不同的需求，必然需要多种推荐算法混合使用。\n",
    "\n",
    "2. 推荐系统是一种信息过滤系统，存在多种评判规则，“用户满意度、预测准确度、覆盖率、多样性、新颖性、惊喜度、实时性、内容时效性、内容质量、商业目标”等等，一种推荐系统不可能满足所有的目标，因此使用多种推荐系统先粗排，然后按照不同的评判标准排序后，输出最后的结果\n",
    "\n",
    "3. 推荐系统存在冷启动问题，静态与动态的不同推荐系统相结合，才能达到满足各类用户的需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 4 : 关联规则中的最小支持度、最小置信度该如何确定?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：\n",
    "1. 支持度表示商品（商品组合）在总体中的出现概率，总体小票数量越大，最小支持度 $ {min}\\_ {support} $ 应该设置的越小，以保证可以存在频繁项集。频繁项集越少时，应当调小最小支持度。\n",
    "\n",
    "2. 置信度表示一个商品（商品组合）的出现，另一商品（商品组合）出现的概率，总体数量越多，最小置信度应当越小。当关联规则太少时，应当调小最小置信度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking 5 : 都有哪些常见的回归分析方法，评价指标是什么?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答：\n",
    "一、 回归分析方法有很多种：\n",
    "1. 参数回归分析方法：\n",
    "    1. 线性回归方法：\n",
    "        1. 一元线性回归/多元线性回归\n",
    "        2. 加权最小二乘回归（解决异方差问题）\n",
    "        3. 岭回归，Lasso回归，Elastic回归分析（解决多重共线性，以及高维变量的变量选择）\n",
    "        4. Logistic回归分析（基于Logistic的一系列分类回归分析方法）\n",
    "        5. Probit、Tobit回归分析（对于受限因变量的回归分析）\n",
    "        6. 针对一系列时间序列的回归分析方法\n",
    "        7. 针对面板数据的一系列回归分析方法\n",
    "        8. 针对空间数据的一系列回归分析方法  \n",
    "        等  \n",
    "    2. 非线性回归分析（可转化为线性回归分析的多项式回归分析等，以及不可转化为线性回归的非线性回归） \n",
    "    \n",
    "    \n",
    "2. 非参数回归分析方法（大样本条件下，不考虑变量的具体分布形式，一般用于解决非线性问题）：\n",
    "    1. 分位数回归分析\n",
    "    2. 中位数回归分析\n",
    "    3. 局部线性回归分析（局部多项式等）\n",
    "    4. 稳健回归\n",
    "    5. 近邻回归（KNN等）\n",
    "    6. 决策树回归分析\n",
    "    7. SVM回归\n",
    "    8. 非参数问题中的：岭回归，Lasso回归，Elastic回归分析  \n",
    "    等  \n",
    "   \n",
    "二、 评价指标：\n",
    "1. MAE（Mean Absolute Error）平均绝对差值\n",
    "2. MSE(Mean Square Error)均方误差，是回归任务最常用的性能度量，最小二乘估计也是使用均方误差\n",
    "3. log对数损失函数（逻辑回归）：交叉熵损失，其实是由最大似然估计推导而来\n",
    "4. RMSE(Root Mean Square error)均方根误差\n",
    "5. Normalized root-mean-square deviation归一化均方差跟偏差\n",
    "6. $ R^2 (以及R^2_{adjust}) $\n",
    "7. Pearson's Correlation Coefficient(皮尔逊相关系数)\n",
    "8. concordance correlation coefficient(一致性相关系数)\n",
    "9. LR似然比检验\n",
    "10. AIC、BIC  \n",
    "等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 资料等\n",
    "https://www.cnblogs.com/sumuncle/p/5647722.html  \n",
    "https://www.zhihu.com/question/19628902  \n",
    "极大似然估计方法：  \n",
    "https://blog.csdn.net/zengxiantao1994/article/details/72787849/  \n",
    "非参数统计理论：  \n",
    "https://baike.baidu.com/item/%E9%9D%9E%E5%8F%82%E6%95%B0%E7%BB%9F%E8%AE%A1/7763503?fr=aladdin  \n",
    "机器学习检验指标：  \n",
    "https://blog.csdn.net/qq_28935065/article/details/84286559  \n",
    "https://blog.csdn.net/tox33/article/details/81141485"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action 针对MarketBasket数据集进行购物篮分析（频繁项集及关联规则挖掘）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Apriori第一种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总计有7501张小票 \n",
      "\n",
      "Wall time: 0 ns\n",
      "频繁项集为：\n",
      " {1: {('frozen smoothie',): 475, ('low fat yogurt',): 574, ('cottage cheese',): 239, ('mineral water',): 1788, ('green tea',): 991, ('avocado',): 250, ('olive oil',): 494, ('tomato juice',): 228, ('salmon',): 319, ('shrimp',): 536, ('honey',): 356, ('burgers',): 654, ('eggs',): 1348, ('turkey',): 469, ('whole wheat rice',): 439, ('milk',): 972, ('french fries',): 1282, ('soup',): 379, ('spaghetti',): 1306, ('frozen vegetables',): 715, ('cookies',): 603, ('cooking oil',): 383, ('champagne',): 351, ('chocolate',): 1229, ('chicken',): 450, ('tomatoes',): 513, ('pancakes',): 713, ('grated cheese',): 393, ('fresh bread',): 323, ('escalope',): 595, ('ground beef',): 737, ('herb & pepper',): 371, ('cake',): 608, ('hot dogs',): 243, ('brownies',): 253, ('butter',): 226}, 2: {('green tea', 'mineral water'): 233, ('milk', 'mineral water'): 360, ('eggs', 'mineral water'): 382, ('eggs', 'spaghetti'): 274, ('mineral water', 'spaghetti'): 448, ('chocolate', 'eggs'): 249, ('mineral water', 'pancakes'): 253, ('milk', 'spaghetti'): 266, ('ground beef', 'mineral water'): 307, ('ground beef', 'spaghetti'): 294, ('chocolate', 'french fries'): 258, ('chocolate', 'mineral water'): 395, ('eggs', 'french fries'): 273, ('french fries', 'mineral water'): 253, ('frozen vegetables', 'mineral water'): 268, ('chocolate', 'spaghetti'): 294, ('chocolate', 'milk'): 241, ('eggs', 'milk'): 231}} \n",
      "\n",
      "关联规则为：\n",
      " [{milk} -> {mineral water}, {spaghetti} -> {mineral water}, {pancakes} -> {mineral water}, {ground beef} -> {mineral water}, {ground beef} -> {spaghetti}, {chocolate} -> {mineral water}, {frozen vegetables} -> {mineral water}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "#加载数据\n",
    "data = pd.read_csv('c:/Users/10109/Documents/Jupyter notebook/人工智能课程(BI方向)/商业智能和推荐系统/lesson2 挖掘数据中的关联规则/homework/Market_Basket_Optimisation.csv', header = None)\n",
    "data = data.fillna(0)\n",
    "#print(data)\n",
    "#将数据整理成Transaction列表\n",
    "transaction = []\n",
    "for i in range(data.shape[0]):\n",
    "    temp = set()\n",
    "    for j in range(data.shape[1]):\n",
    "        if data.iloc[i, j] != 0:\n",
    "            temp.add(data.iloc[i, j])\n",
    "    transaction.append(temp)\n",
    "# print(transaction)\n",
    "print('总计有{}张小票'.format(data.shape[0]),'\\n')\n",
    "itemsets, rules = apriori(transaction, min_support = 0.03, min_confidence = 0.3)\n",
    "\n",
    "print('频繁项集为：\\n', itemsets,'\\n')\n",
    "print('关联规则为：\\n', rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Apriori第二种方法输出详细结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Wall time: 0 ns\n",
      "总计有7501张小票 \n",
      "\n",
      "频繁项集为：\n",
      "      support                    itemsets\n",
      "0   0.087188                   (burgers)\n",
      "1   0.081056                      (cake)\n",
      "2   0.059992                   (chicken)\n",
      "3   0.163845                 (chocolate)\n",
      "4   0.080389                   (cookies)\n",
      "5   0.051060               (cooking oil)\n",
      "6   0.179709                      (eggs)\n",
      "7   0.079323                  (escalope)\n",
      "8   0.170911              (french fries)\n",
      "9   0.063325           (frozen smoothie)\n",
      "10  0.095321         (frozen vegetables)\n",
      "11  0.052393             (grated cheese)\n",
      "12  0.132116                 (green tea)\n",
      "13  0.098254               (ground beef)\n",
      "14  0.076523            (low fat yogurt)\n",
      "15  0.129583                      (milk)\n",
      "16  0.238368             (mineral water)\n",
      "17  0.065858                 (olive oil)\n",
      "18  0.095054                  (pancakes)\n",
      "19  0.071457                    (shrimp)\n",
      "20  0.050527                      (soup)\n",
      "21  0.174110                 (spaghetti)\n",
      "22  0.068391                  (tomatoes)\n",
      "23  0.062525                    (turkey)\n",
      "24  0.058526          (whole wheat rice)\n",
      "25  0.052660  (chocolate, mineral water)\n",
      "26  0.050927       (eggs, mineral water)\n",
      "27  0.059725  (spaghetti, mineral water) \n",
      "\n",
      "关联规则为：\n",
      "        antecedents      consequents  antecedent support  consequent support  \\\n",
      "0      (chocolate)  (mineral water)            0.163845            0.238368   \n",
      "1  (mineral water)      (chocolate)            0.238368            0.163845   \n",
      "2           (eggs)  (mineral water)            0.179709            0.238368   \n",
      "3  (mineral water)           (eggs)            0.238368            0.179709   \n",
      "4      (spaghetti)  (mineral water)            0.174110            0.238368   \n",
      "5  (mineral water)      (spaghetti)            0.238368            0.174110   \n",
      "\n",
      "    support  confidence      lift  leverage  conviction  \n",
      "0  0.052660    0.321400  1.348332  0.013604    1.122357  \n",
      "1  0.052660    0.220917  1.348332  0.013604    1.073256  \n",
      "2  0.050927    0.283383  1.188845  0.008090    1.062815  \n",
      "3  0.050927    0.213647  1.188845  0.008090    1.043158  \n",
      "4  0.059725    0.343032  1.439085  0.018223    1.159314  \n",
      "5  0.059725    0.250559  1.439085  0.018223    1.102008  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from pandas import DataFrame\n",
    "\n",
    "#加载数据\n",
    "data = pd.read_csv('c:/Users/10109/Documents/Jupyter notebook/人工智能课程(BI方向)/商业智能和推荐系统/lesson2 挖掘数据中的关联规则/homework/Market_Basket_Optimisation.csv', header = None)\n",
    "data = data.fillna(0)\n",
    "\n",
    "#将数据整理成Transaction列表\n",
    "transaction = []\n",
    "for i in range(data.shape[0]):\n",
    "    temp = str()\n",
    "    for j in range(data.shape[1]):\n",
    "        if data.iloc[i, j] != 0:\n",
    "            temp += str(data.iloc[i, j]) + str(',')\n",
    "    transaction.append(temp)\n",
    "\n",
    "transactions = DataFrame({'Item':transaction})\n",
    "one_hot = transactions.drop('Item', 1).join(transactions.Item.str.get_dummies(','))\n",
    "\n",
    "#使用onehot数据进行关联分析\n",
    "frequent = apriori(one_hot, min_support = 0.05, use_colnames = True)\n",
    "rules = association_rules(frequent, metric = 'lift', min_threshold = 1)\n",
    "\n",
    "print('总计有{}张小票'.format(one_hot.shape[0]),'\\n')\n",
    "print('频繁项集为：\\n', frequent,'\\n')\n",
    "print('关联规则为：\\n', rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用spark 中的 FPGrowth方法  \n",
    ">### FPGrowth 源码构造方式\n",
    "https://zhuanlan.zhihu.com/p/140745153?utm_source=wechat_session&utm_medium=social&utm_oi=936633886726053888   \n",
    ">### pyspark FPGrowth 使用方式\n",
    "https://blog.csdn.net/qq_23860475/article/details/90748080?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               items|freq|\n",
      "+--------------------+----+\n",
      "|          [hot dogs]| 243|\n",
      "| [frozen vegetables]| 715|\n",
      "|[frozen vegetable...| 268|\n",
      "|           [chicken]| 450|\n",
      "|         [chocolate]|1229|\n",
      "|[chocolate, spagh...| 294|\n",
      "|   [chocolate, eggs]| 249|\n",
      "|[chocolate, frenc...| 258|\n",
      "|[chocolate, miner...| 395|\n",
      "|          [tomatoes]| 513|\n",
      "+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------------+---------------+-------------------+------------------+\n",
      "|     antecedent|     consequent|         confidence|              lift|\n",
      "+---------------+---------------+-------------------+------------------+\n",
      "|         [milk]|    [spaghetti]| 0.2736625514403292|1.5717785592296396|\n",
      "|         [milk]|    [chocolate]|0.24794238683127573|  1.51327570677087|\n",
      "|         [milk]|         [eggs]|0.23765432098765432| 1.322436989412756|\n",
      "|         [milk]|[mineral water]|0.37037037037037035|1.5537741320739082|\n",
      "|[mineral water]|    [chocolate]|  0.220917225950783| 1.348332068231752|\n",
      "+---------------+---------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "强关联规则：\n",
      "              antecedent       consequent  confidence      lift\n",
      "14          [spaghetti]    [ground beef]    0.225115  2.291162\n",
      "20        [ground beef]      [spaghetti]    0.398915  2.291162\n",
      "21        [ground beef]  [mineral water]    0.416554  1.747522\n",
      "16  [frozen vegetables]  [mineral water]    0.374825  1.572463\n",
      "13          [spaghetti]           [milk]    0.203675  1.571779\n",
      "0                [milk]      [spaghetti]    0.273663  1.571779\n",
      "7       [mineral water]           [milk]    0.201342  1.553774\n",
      "3                [milk]  [mineral water]    0.370370  1.553774\n",
      "1                [milk]      [chocolate]    0.247942  1.513276\n",
      "22           [pancakes]  [mineral water]    0.354839  1.488616\n",
      "6       [mineral water]      [spaghetti]    0.250559  1.439085\n",
      "12          [spaghetti]  [mineral water]    0.343032  1.439085\n",
      "23          [chocolate]      [spaghetti]    0.239219  1.373952\n",
      "10          [spaghetti]      [chocolate]    0.225115  1.373952\n",
      "26          [chocolate]  [mineral water]    0.321400  1.348332\n",
      "4       [mineral water]      [chocolate]    0.220917  1.348332\n",
      "2                [milk]           [eggs]    0.237654  1.322437\n",
      "25          [chocolate]   [french fries]    0.209927  1.228284\n",
      "8        [french fries]      [chocolate]    0.201248  1.228284\n",
      "5       [mineral water]           [eggs]    0.213647  1.188845\n",
      "17               [eggs]  [mineral water]    0.283383  1.188845\n",
      "19               [eggs]   [french fries]    0.202522  1.184961\n",
      "9        [french fries]           [eggs]    0.212949  1.184961\n",
      "18               [eggs]      [spaghetti]    0.203264  1.167446\n",
      "11          [spaghetti]           [eggs]    0.209801  1.167446\n",
      "24          [chocolate]           [eggs]    0.202604  1.127397\n",
      "15          [green tea]  [mineral water]    0.235116  0.986357\n",
      "后项预测：\n",
      " ['spaghetti', 'chocolate', 'eggs', 'mineral water']\n",
      "spent ts: 0:00:18.254097\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "#加载数据\n",
    "data = pd.read_csv('c:/Users/10109/Documents/Jupyter notebook/人工智能课程(BI方向)/商业智能和推荐系统/lesson2 挖掘数据中的关联规则/homework/Market_Basket_Optimisation.csv', header = None)\n",
    "data = data.fillna(0)\n",
    "#print(data)\n",
    "#将数据整理成Transaction列表\n",
    "transaction = []\n",
    "for i in range(data.shape[0]):\n",
    "    temp = set()\n",
    "    for j in range(data.shape[1]):\n",
    "        if data.iloc[i, j] != 0:\n",
    "            temp.add(data.iloc[i, j])\n",
    "    transaction.append(temp)\n",
    "\n",
    "transactions = []\n",
    "for i in transaction:\n",
    "    temp_list = []\n",
    "    for j in i:\n",
    "        temp_list.append(j)\n",
    "    transactions.append([temp_list])\n",
    "    \n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "import datetime\n",
    "if __name__ == \"__main__\":\n",
    "    t1=datetime.datetime.now()\n",
    "    appname = \"FPgrowth\"\n",
    "    master =\"local[4]\" \n",
    "\n",
    "    #spark配置\n",
    "    conf = SparkConf().setAppName(appname).setMaster(master)                  \n",
    "    spark = SparkSession.builder.config(conf = conf).getOrCreate()\n",
    "    \n",
    "    #加载数据\n",
    "    data = transactions    \n",
    "    #将数据转为spark中的dataframe\n",
    "    data = spark.createDataFrame(data, [\"items\"])\n",
    "    \n",
    "    #模型建立\n",
    "    fp = FPGrowth(minSupport=0.03, minConfidence=0.2)\n",
    "    #模型拟合\n",
    "    fpm  = fp.fit(data)\n",
    "    #在控制台显示前五条频繁项集\n",
    "    fpm.freqItemsets.show(10)\n",
    "    #强关联规则\n",
    "    assRule=fpm.associationRules\n",
    "    assRule.show(5)\n",
    "    \n",
    "    #转为python中的dataframe\n",
    "    assRuleDf = assRule.toPandas()  \n",
    "    #由 lift 按照降序排列\n",
    "    assRuleDf = assRuleDf.sort_values(by = \"lift\", ascending = False)\n",
    "    print('强关联规则：\\n',assRuleDf)\n",
    "    \n",
    "    #新的前项数据\n",
    "    new_data = spark.createDataFrame([([\"milk\"], )], [\"items\"])\n",
    "    #预测后项\n",
    "    print('后项预测：\\n',fpm.transform(new_data).first().prediction)               \n",
    "    spark.stop()#关闭spark\n",
    "    t2=datetime.datetime.now()\n",
    "    print('spent ts:',t2-t1)\n",
    "    #遇到 'NoneType' object has no attribute 'setCallSite' 记得 restart kernel\n",
    "    \n",
    "    #接收data类型为如下格式：\n",
    "    \n",
    "    #     data_list=[[['r', 'z', 'h', 'k', 'p']]\\\n",
    "#                ,[['z', 'y', 'x', 'w', 'v', 'u', 't', 's']]\\\n",
    "#                ,[['s', 'x', 'o', 'n', 'r']]\\\n",
    "#                ,[['x', 'z', 'y', 'm', 't', 's', 'q', 'e']]\\\n",
    "#                ,[['z']]\\\n",
    "#                ,[['x', 'z', 'y', 'r', 'q', 't', 'p']]]#数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 购物篮课程代码复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有9684张小票\n",
      "频繁项集:\n",
      " {1: {('Cookies',): 515, ('Hot chocolate',): 552, ('Muffin',): 364, ('Coffee',): 4528, ('Bread',): 3096, ('Pastry',): 815, ('Medialuna',): 585, ('Tea',): 1350, ('Farm House',): 371, ('Juice',): 365, ('Soup',): 326, ('Cake',): 983, ('Sandwich',): 680, ('Alfajores',): 344, ('Brownie',): 379, ('Toast',): 318, ('Scone',): 327}, 2: {('Bread', 'Coffee'): 852, ('Coffee', 'Pastry'): 450, ('Coffee', 'Medialuna'): 333, ('Coffee', 'Tea'): 472, ('Cake', 'Coffee'): 518, ('Coffee', 'Sandwich'): 362}}\n",
      "关联规则:\n",
      " [{Coffee} -> {Bread}, {Bread} -> {Coffee}, {Pastry} -> {Coffee}, {Medialuna} -> {Coffee}, {Tea} -> {Coffee}, {Coffee} -> {Tea}, {Coffee} -> {Cake}, {Cake} -> {Coffee}, {Sandwich} -> {Coffee}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "#加载数据\n",
    "data = pd.read_csv('BreadBasket_DMS.csv')\n",
    "data\n",
    "\n",
    "#注意到Item中含有NONE\n",
    "data = data.drop(data[data.Item == 'NONE'].index)\n",
    "order_set = data.set_index('Transaction')['Item']\n",
    "\n",
    "#将数据转换为可使用的格式\n",
    "transaction = []\n",
    "temp_i = 0\n",
    "for i, v in order_set.items():\n",
    "    if i != temp_i:\n",
    "        temp_i = i\n",
    "        temp_set = set()\n",
    "        temp_set.add(v)\n",
    "        transaction.append(temp_set)\n",
    "        #Python中list拼接的是一个变量，变量是一个关于对象的引用，因此list中其实是一个个指针构成的，指向位置的内存发生改变就会导致list变化\n",
    "    else:\n",
    "        temp_set.add(v)\n",
    "transaction\n",
    "\n",
    "#使用关联规则对Transaction进行数据挖掘\n",
    "print('共有{}张小票'.format(data['Transaction'].max()))\n",
    "itemsets, rules = apriori(transaction, min_support = 0.03, min_confidence = 0.1)\n",
    "print('频繁项集:\\n', itemsets)\n",
    "print('关联规则:\\n', rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "频繁项集：\n",
      "      support             itemsets\n",
      "0   0.036348          (Alfajores)\n",
      "1   0.327134              (Bread)\n",
      "2   0.040046            (Brownie)\n",
      "3   0.103867               (Cake)\n",
      "4   0.478445             (Coffee)\n",
      "5   0.054417            (Cookies)\n",
      "6   0.039201         (Farm House)\n",
      "7   0.058326      (Hot chocolate)\n",
      "8   0.038567              (Juice)\n",
      "9   0.061813          (Medialuna)\n",
      "10  0.038462             (Muffin)\n",
      "11  0.086116             (Pastry)\n",
      "12  0.071851           (Sandwich)\n",
      "13  0.034552              (Scone)\n",
      "14  0.034446               (Soup)\n",
      "15  0.142646                (Tea)\n",
      "16  0.033601              (Toast)\n",
      "17  0.090025      (Coffee, Bread)\n",
      "18  0.054734       (Coffee, Cake)\n",
      "19  0.035186  (Coffee, Medialuna)\n",
      "20  0.047549     (Pastry, Coffee)\n",
      "21  0.038250   (Coffee, Sandwich)\n",
      "22  0.049873        (Tea, Coffee) \n",
      "\n",
      "关联规则：\n",
      "    antecedents  consequents  antecedent support  consequent support   support  \\\n",
      "0     (Coffee)       (Cake)            0.478445            0.103867  0.054734   \n",
      "1       (Cake)     (Coffee)            0.103867            0.478445  0.054734   \n",
      "2     (Coffee)  (Medialuna)            0.478445            0.061813  0.035186   \n",
      "3  (Medialuna)     (Coffee)            0.061813            0.478445  0.035186   \n",
      "4     (Pastry)     (Coffee)            0.086116            0.478445  0.047549   \n",
      "5     (Coffee)     (Pastry)            0.478445            0.086116  0.047549   \n",
      "6     (Coffee)   (Sandwich)            0.478445            0.071851  0.038250   \n",
      "7   (Sandwich)     (Coffee)            0.071851            0.478445  0.038250   \n",
      "\n",
      "   confidence      lift  leverage  conviction  \n",
      "0    0.114399  1.101399  0.005039    1.011893  \n",
      "1    0.526958  1.101399  0.005039    1.102557  \n",
      "2    0.073542  1.189753  0.005612    1.012660  \n",
      "3    0.569231  1.189753  0.005612    1.210754  \n",
      "4    0.552147  1.154046  0.006347    1.164569  \n",
      "5    0.099382  1.154046  0.006347    1.014730  \n",
      "6    0.079947  1.112674  0.003873    1.008799  \n",
      "7    0.532353  1.112674  0.003873    1.115276  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "#加载数据\n",
    "data = pd.read_csv('BreadBasket_DMS.csv')\n",
    "data = data.drop(data[data.Item == 'NONE'].index)\n",
    "\n",
    "#将数据变为ONEHOT编码\n",
    "transaction = data.groupby(['Transaction', 'Item']).Item.count().unstack().fillna(0)\n",
    "\n",
    "#数据为float数，转换为0，1编码\n",
    "def encode_units(x):\n",
    "    if x <= 0 :\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "        \n",
    "transaction = transaction.applymap(lambda x: encode_units(x))\n",
    "pd.options.display.max_columns=100\n",
    "transaction\n",
    "\n",
    "#使用关联规则进行数据挖掘\n",
    "frequentset = apriori(transaction, min_support = 0.03, use_colnames = True)\n",
    "rules = association_rules(frequentset, metric = 'lift', min_threshold = 1)\n",
    "\n",
    "print('频繁项集：\\n', frequentset,'\\n')\n",
    "print('关联规则：\\n', rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
